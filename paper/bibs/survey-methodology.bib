@article{Zagheni2017,
author = {Zagheni, Emilio and Weber, Ingmar and Gummadi, Krishna},
doi = {10.1111/padr.12102},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zagheni, Weber, Gummadi - 2017 - Leveraging Facebook's Advertising Platform to Monitor Stocks of Migrants.pdf:pdf},
issn = {17284457},
journal = {Population and Development Review},
number = {4},
pages = {721--734},
title = {{Leveraging Facebook's Advertising Platform to Monitor Stocks of Migrants}},
volume = {43},
year = {2017}
}
@article{Lindhjem2011,
abstract = {Internet is quickly becoming the survey mode of choice for stated preference (SP) surveys in environmental economics. However, this choice is being made with relatively little consideration of its potential influence on survey results. This paper reviews the theory and emerging evidence of mode effects in the survey methodology and SP literatures, summarizes the findings, and points out implications for Internet SP practice and research. The SP studies that compare Internet with other modes do generally not find substantial difference. The majority of welfare estimates are equal; or somewhat lower for the Internet surveys. Further, there is no clear evidence of substantially lower quality or validity of Internet responses. However, the degree of experimental control is often low in comparative studies across survey modes, and they often confound measurement and sample composition effects. Internet offers a huge potential for experimentation and innovation in SP research, but when used to derive reliable welfare estimates for policy assessment, issues like representation and nonresponse bias for different Internet panels should receive more attention. {\textcopyright} 2011 H. Lindhjem and S. Navrud.},
author = {Lindhjem, Henrik and Navrud, St{\aa}le},
doi = {10.1561/101.00000045},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindhjem, Navrud - 2011 - Using internet in stated preference surveys A review and comparison of survey modes.pdf:pdf},
issn = {19321465},
journal = {International Review of Environmental and Resource Economics},
keywords = {Contingent valuation,Internet,Stated preferences,Survey mode},
number = {4},
pages = {309--351},
title = {{Using internet in stated preference surveys: A review and comparison of survey modes}},
volume = {5},
year = {2011}
}
@article{Grow2020,
author = {Grow, Andr{\'{e}} and Perrotta, Daniela and {Del Fava}, Emanuele and Cimentada, Jorge and Rampazzo, Francesco and Gil-Clavel, Sofia and Zagheni, Emilio},
doi = {10.31235/osf.io/ez9pb},
file = {:home/nandan/Downloads/_papers/Addressing Public Health Emergencies via Facebook Surveys.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
title = {{Addressing Public Health Emergencies via Facebook Surveys: Advantages, Challenges, and Practical Considerations}},
year = {2020}
}
@article{Battaglia2009,
abstract = {One can use raking to improve the relation between the sample and the population by adjusting the sampling weights of the cases in the sample so that the marginal totals of the adjusted weights on specified characteristics agree with the corresponding totals for the population. The raking procedure is described, and convergence issues and problems are discussed. The},
author = {Battaglia, Michael P. and Hoaglin, David C. and Frankel, Martin R.},
doi = {10.29115/sp-2009-0019},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2953-practical-considerations-in-raking-survey-data.pdf:pdf},
issn = {2168-0094},
journal = {Survey Practice},
number = {5},
pages = {1--10},
title = {{Practical Considerations in Raking Survey Data}},
volume = {2},
year = {2009}
}
@article{Neyman1934,
abstract = {Describes the mathod of random sampling and the method of purposive.},
author = {Neyman, Jerzy},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/2342192.pdf:pdf},
issn = {0952-8385},
journal = {Journal of the Royal Statistical Society},
number = {4},
pages = {558--625},
title = {{On the Two Different Aspects of the Representative Method : The Method of Stratified Sampling and the Method of Purposive Selection Author ( s ): Jerzy Neyman Source : Journal of the Royal Statistical Society , Vol . 97 , No . 4 ( 1934 ), pp . 558-625 Pub}},
volume = {97},
year = {1934}
}
@article{Keeter2017,
abstract = {Telephone polls still provide accurate data on a wide range of social, demographic and political variables, but some weaknesses persist.},
author = {Keeter, Scott and Hatley, Nick and Kennedy, Courtney and Lau, Arnold},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Keeter et al. - 2017 - What Low Response Rates Mean for Telephone Surveys.pdf:pdf},
pages = {1--39},
title = {{What Low Response Rates Mean for Telephone Surveys}},
url = {http://www.pewresearch.org/wp-content/uploads/2017/05/RDD-Non-response-Full-Report.pdf},
year = {2017}
}
@article{Mellon2017,
abstract = {A growing social science literature has used Twitter and Facebook to study political and social phenomena including for election forecasting and tracking political conversations. This research note uses a nationally representative probability sample of the British population to examine how Twitter and Facebook users differ from the general population in terms of demographics, political attitudes and political behaviour. We find that Twitter and Facebook users differ substantially from the general population on many politically relevant dimensions including vote choice, turnout, age, gender, and education. On average social media users are younger and better educated than non-users, and they are more liberal and pay more attention to politics. Despite paying more attention to politics, social media users are less likely to vote than non-users, but they are more likely to support the left leaning Labour Party when they do vote. However, we show that these apparent differences mostly arise due to the demographic composition of social media users. After controlling for age, gender, and education, no statistically significant differences arise between social media users and non-users on political attention, values or political behaviour.},
author = {Mellon, Jonathan and Prosser, Christopher},
doi = {10.1177/2053168017720008},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mellon, Prosser - 2017 - Twitter and Facebook are not representative of the general population Political attitudes and demographics of b.pdf:pdf},
issn = {20531680},
journal = {Research and Politics},
keywords = {British election study,Election forecasting,Facebook,Representativeness,Social media,Twitter},
number = {3},
pages = {1--9},
title = {{Twitter and Facebook are not representative of the general population: Political attitudes and demographics of british social media users}},
volume = {4},
year = {2017}
}
@book{Chambliss2019,
author = {Chambliss, Daniel F. and Schutt, Russel K.},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chambliss, Schutt - 2019 - Making Sense of the Social World Methods of Investigation.pdf:pdf},
title = {{Making Sense of the Social World: Methods of Investigation}},
year = {2019}
}
@article{Toepoel2020,
abstract = {Online surveys are increasingly completed on smartphones. There are several ways to structure online surveys so as to create an optimal experience for any screen size. For example, communicating through applications (apps) such as WhatsApp and Snapchat closely resembles natural turn-by-turn conversations between individuals. Web surveys currently mimic the design of paper questionnaires mostly, leading to a survey experience that may not be optimal when completed on smartphones. In this paper, we compare a research messenger design, which mimics a messenger app type of communication, to a responsive survey design. We investigate whether response quality is similar between the two designs and whether respondents' satisfaction with the survey is higher for either version. Our results show no differences for primacy effects, number of nonsubstantive answers, and dropout rate. The length of open-ended answers was shorter for the research messenger survey compared to the responsive design, and the overall time of completion was longer in the research messenger survey. The evaluation at the end of the survey showed no clear indication that respondents liked the research messenger survey more than the responsive design. Future research should focus on how to optimally design online mixed-device surveys in order to increase respondent satisfaction and data quality.},
author = {Toepoel, Vera and Lugtig, Peter and Struminskaya, Bella and Elevelt, Anne and Haan, Marieke},
doi = {10.29115/sp-2020-0010},
file = {:home/nandan/Downloads/_papers/14188-adapting-surveys-to-the-modern-world-comparing-a-research-messenger-design-to-a-regular-responsive-design-for-online-surveys.pdf:pdf},
journal = {Survey Practice},
keywords = {10,29115,data quality,doi,https,mixed-device survey,mobile survey,org,research messenger,respondent burden,sp-2020-0010},
number = {1},
pages = {1--10},
title = {{Adapting surveys to the modern world: Comparing a research messenger design to a regular responsive design for online surveys}},
volume = {13},
year = {2020}
}
@article{Tourangeau2013,
abstract = {The development and widespread use of Web surveys have resulted in an outpouring of research on their design. In this volume, Tourangeau, Conrad, and Couper provide a comprehensive summary and synthesis of the literature on this increasingly popular method of data collection. The book includes new integration of the authors' work with other important research on Web surveys, including a meta-analysis of studies that compare reports on sensitive topics in Web surveys with reports collected in other modes of data collection. Adopting the total survey error framework, the book examines sampling and coverage issues, nonresponse, measurement, and the issues involved in combining modes. In addition, the concluding chapter provides a model for understanding the errors in estimates that combine data collected in more than one mode. Web surveys have several important characteristics that affect their ability to collect accurate survey data. Discussing these in detail, the authors address basic design decisions from input widgets to background colors. They additionally focus on the visual character of Web surveys, on their ability to automatically interact with respondents, and on the Web as a method of self-administration. The Science of Web Surveys is relevant for those with the practical goal of improving their own surveys and those with an interest in understanding an increasingly important method of data collection.},
author = {Tourangeau, Roger and Conrad, Frederick G. and Couper, Mick P.},
doi = {10.1093/acprof:oso/9780199747047.001.0001},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tourangeau, Conrad, Couper - 2013 - The Science of Web Surveys.pdf:pdf},
journal = {The Science of Web Surveys},
title = {{The Science of Web Surveys}},
year = {2013}
}
@book{Polling,
abstract = {polls - n{\~{a}}o interessa},
author = {Polling, Political and Salt, The and Tribune, Lake},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Polling, Salt, Tribune - Unknown - Political Polling in the Digital Age.pdf:pdf},
isbn = {9780807138298},
title = {{Political Polling in the Digital Age}}
}
@article{Gelman1997,
author = {Gelman, Andrew and Little, Thomas C.},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/poststrat3.pdf:pdf},
keywords = {bayesian inference,election forecasting,nonresponse,opinion polls,sample sur-},
title = {{Poststratification Into Many Categories Using Hierarchical Logistic Regression}},
year = {1997}
}
@article{Krueger2014,
abstract = {Given the theoretical promise of these auxiliary data for overcoming the challenge of nonresponse, survey researchers have shown an increased interest in collecting paradata, data from screening interviews, and other contextual information. However, very few studies have systematically assessed the use of these data for post-survey nonresponse adjustments. Those studies that do exist generally do not identify auxiliary variables that correlate with response propensity and key survey variables, a necessary condition for auxiliary data to be effective tools for reducing nonresponse bias. Using the National Survey of Family Growth (NSFG), this paper leverages a large set of auxiliary variables available for the full NSFG sample to assess their potential as independent tools for post-survey nonresponse adjustments. We begin by using this auxiliary information to predict response propensity (RP) for each person in the full sample. We then display descriptive estimates for a variety of attitudes and behaviors measured in the NSFG, using post-stratification weighting adjustments as well as RP adjustments followed by post-stratification. The results show that accounting for RP in the weighting adjustment often produces noteworthy differences in the estimates, thus supporting the collection of these types of auxiliary variables in practice. These results also suggest that standard post-stratification adjustments may not be entirely effective at removing nonresponse bias from all survey estimates, and that some subgroup analyses may be especially subject to bias when adjusting survey estimates using post-stratification alone.},
author = {Krueger, B. S. and West, B. T.},
doi = {10.1093/poq/nfu040},
file = {:home/nandan/Downloads/_papers/nfu040.pdf:pdf},
issn = {0033-362X},
journal = {Public Opinion Quarterly},
number = {4},
pages = {795--831},
title = {{Assessing the Potential of Paradata and Other Auxiliary Data for Nonresponse Adjustments}},
volume = {78},
year = {2014}
}
@article{VanDenBrakel2017,
abstract = {In this paper the question is addressed how alternative data sources, such as administrative and social media data, can be used in the production of official statistics. Since most surveys at national statistical institutes are conducted repeatedly over time, a multivariate structural time series modelling approach is proposed to model the series observed by a repeated surveys with related series obtained from such alternative data sources. Generally, this improves the precision of the direct survey estimates by using sample information observed in preceding periods and information from related auxiliary series. This model also makes it possible to utilize the higher frequency of the social media to produce more precise estimates for the sample survey in real time at the moment that statistics for the social media become available but the sample data are not yet available. The concept of cointegration is applied to address the question to which extent the alternative series represent the same phenomena as the series observed with the repeated survey. The methodology is applied to the Dutch Consumer Confidence Survey and a sentiment index derived from social media.},
author = {{Van Den Brakel}, Jan and S{\"{o}}hler, Emily and Daas, Piet and Buelens, Bart},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Den Brakel et al. - 2017 - Social media as a data source for official statistics the dutch consumer confidence index.pdf:pdf},
issn = {14920921},
journal = {Survey Methodology},
keywords = {Big data,Cointegration,Design-based inference,Model-based inference,Nowcasting,Structural time series modelling},
number = {2},
pages = {183--210},
title = {{Social media as a data source for official statistics; the dutch consumer confidence index}},
volume = {43},
year = {2017}
}
@article{DeVellis2016,
abstract = {Scale Development: Theory and Applications has helped students, practitioners, and researchers design and develop scales. In this updated version, DeVellis has expanded his coverage of factor analysis and has added a new chapter on item response theory.},
author = {DeVellis, Robert F},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DeVellis - 2016 - Scale Development Theory and Applications ( Fourth Edition Robert).pdf:pdf},
journal = {SAGE Publication},
pages = {256},
title = {{Scale Development Theory and Applications ( Fourth Edition Robert)}},
url = {https://b-ok.cc},
volume = {4},
year = {2016}
}
@article{Czajka2016,
abstract = {Key Findings: *  A long-term decline in survey response rates has accelerated in recent years. *  The increase in nonresponse has been much greater in telephone surveys than in face-to-face surveys. *  Response rates are a poor predictor of nonresponse bias, which tends to be item-specific. *  Payment of incentives is the most effective strategy to increase response rates. This report reviews recent assessments of trends in response rates among primarily federal surveys and the reasons for declining response rates; documents response trends in seven major Health and Human Services surveys and the Census Bureau's Current Population Survey from the mid-1990s through the most recent year available; examines what is known about the relationship between response rates and nonresponse bias; reviews approaches to addressing nonresponse and its effects; and summarizes key conclusions.},
author = {Czajka, John L. and Beyler, Amy},
file = {:home/nandan/Downloads/_papers/Decliningresponserates.pdf:pdf},
journal = {Mathematica Policy Research},
keywords = {cross-sectional s,nonresponse bias,survey burden},
number = {202},
pages = {1--54},
title = {{Declining Response Rates in Federal Surveys: Trends and Implications}},
url = {https://www.mathematica-mpr.com/our-publications-and-findings/publications/declining-response-rates-in-federal-surveys-trends-and-implications-background-paper},
volume = {I},
year = {2016}
}
@article{Perrotta2020,
abstract = {In the absence of medical treatment and vaccination, the mitigation and containment of the ongoing COVID-19 pandemic relies on behavioral changes. Timely data on attitudes and behaviors are thus necessary to develop optimal intervention strategies and to assess the consequences of the pandemic for different demographic groups. We developed a rapid response monitoring system via a continuously run online survey (the "COVID-19 Health Behavior Survey") across eight countries (Belgium, France, Germany, Italy, the Netherlands, Spain, the United Kingdom, the United States). The survey was specifically designed to collect key information on people&#039;s health status, behaviors, close social contacts, and attitudes in response to the COVID-19 pandemic. We developed an innovative approach to recruit participants via targeted Facebook advertisement campaigns in order to generate balanced samples for post-stratification. In this paper, we present results for the period from March 13-April 19, 2020. We estimate important differences by sex: women show a substantially higher perception of threat along with a lower level of confidence in the health system. This is paralleled by sex-specific behaviors, with women more likely to adopt a wide range of preventive behaviors. We thus expect behavior to serve as a protective factor for women. Our findings also show a higher level of awareness and concern among older respondents, in line with the evidence that the elderly are at highest risk of severe complications following infection from COVID-19. While across all the samples respondents were less concerned for themselves than for their country or for the world, we also observed substantial temporal and spatial heterogeneity in terms of confidence in institutions and responses to non-pharmaceutical interventions.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis study was funded through the support of the Max Planck Institute for Demographic Research, which is part of the Max Planck Society.Author DeclarationsAll relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.YesAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesDue to data protection regulations, the data are not publicly available. For queries about the data, please contact the corresponding authors.},
author = {Perrotta, Daniela and Grow, Andr{\'{e}} and Rampazzo, Francesco and Cimentada, Jorge and {Del Fava}, Emanuele and Gil-Clavel, Sofia and Zagheni, Emilio},
doi = {10.1101/2020.05.09.20096388},
file = {:home/nandan/Downloads/_papers/2020.05.09.20096388v2.full.pdf:pdf},
pages = {1--17},
title = {{Behaviors and attitudes in response to the COVID-19 pandemic: Insights from a cross-national Facebook survey}},
year = {2020}
}
@article{Tourangeau2010,
abstract = {Survey researchers have long speculated that there may be a link between nonresponse and measurement error - that is, people likely to become nonrespondents to a survey are also likely to make poor reporters if they do take part. Still, there is surprisingly little evidence of such a link. It could be that nonresponse is generally the product of one set of factors and reporting errors, the product of an unrelated set, or both nonresponse and reporting errors may be item-specific so that no general relationship between the two is likely to emerge. Our study examined a situation in which we thought there would be a link between response propensities and the propensity to give inaccurate answers. We asked samples of voters and nonvoters to take part in a survey that included items about voting. Past research shows that nonvoters misreport that fact and that they are less likely than voters in general to take part in surveys. We thought we could heighten the differences between voters and nonvoters in both response rates and levels of misreporting if we characterized the survey as being about politics. However, only nonresponse biases were larger when the topic of the survey was described as political, and this difference was only marginally significant. These two ways of framing the study had even smaller effects on estimates derived from other items in the questionnaire. The overall biases in estimates derived from the voting items are very substantial, and both nonresponse and measurement error contribute to them. {\textcopyright} The Author 2010. Published by Oxford University Press on behalf of the American Association for Public Opinion Research. All rights reserved.},
author = {Tourangeau, Roger and Groves, Robert M. and Redline, Cleo D.},
doi = {10.1093/poq/nfq004},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tourangeau, Groves, Redline - 2010 - Sensitive topics and reluctant respondents Demonstrating a link between nonresponse bias and measur.pdf:pdf},
issn = {0033362X},
journal = {Public Opinion Quarterly},
number = {3},
pages = {413--432},
title = {{Sensitive topics and reluctant respondents: Demonstrating a link between nonresponse bias and measurement error}},
volume = {74},
year = {2010}
}
@article{Goel2015,
abstract = {Probability-based sampling methods, such as random-digit dialing (RDD) of phones, are a staple of modern survey research and have been successfully used to gauge public opinion for sixty years. Though historically eﬀective, this class of traditional survey techniques are often slow and expensive. At the same time, it has become increasingly quick and costeﬀective to collect non-probability-based convenience samples, such as opt-in samples online. Here we investigate the potential of such nonrepresentative data for survey research by administering an online, fully opt-in poll of social and political attitudes. Our survey consisted of 49 multiple-choice attitudinal questions drawn from the probability-based, in-person 2012 General Social Survey (GSS) and select RDD phone surveys by the Pew Research Center. To correct for the inherent biases of non-representative data, we statistically adjust estimates via model-based poststratiﬁcation. Compared to typical RDD phone polls, the opt-in online survey required less than one-tenth the time and money to conduct. After statistical correction, we ﬁnd the median absolute diﬀerence between the non-probability-based online survey and the probability-based GSS and Pew studies is 7.4 percentage points. Though this diﬀerence is considerably larger than if the surveys were all perfect simple random samples, we ﬁnd the gap is comparable to that between the GSS and Pew estimates themselves, ostensibly because even the best available surveys suﬀer from substantial non-sampling error. We conclude that non-representative surveys are a promising tool for fast, cheap, and (mostly) accurate measurement of public opinion.},
author = {Goel, Sharad and Obeng, Adam and Rothschild, David},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goel, Obeng, Rothschild - 2015 - Non-Representative Surveys Fast, Cheap, and Mostly Accurate.pdf:pdf},
pages = {27},
title = {{Non-Representative Surveys: Fast, Cheap, and Mostly Accurate}},
year = {2015}
}
@article{Groves2010a,
abstract = {"Total survey error" is a conceptual framework describing statistical error properties of sample survey statistics. Early in the history of sample surveys, it arose as a tool to focus on implications of various gaps between the conditions under which probability samples yielded unbiased estimates of finite population parameters and practical situations in implementing survey design. While the framework permits design-based estimates of various error components, many of the design burdens to produce those estimates are large, and in practice most surveys do not implement them. Further, the framework does not incorporate other, nonstatistical, dimensions of quality that are commonly utilized in evaluating statistical information. The importation of new modeling tools brings new promise to measuring total survey error components, but also new challenges. A lasting value of the total survey error framework is at the design stage of a survey, to attempt a balance of costs and various errors. Indeed, this framework is the central organizing structure of the field of survey methodology. {\textcopyright} The Author 2011.},
author = {Groves, Robert M. and Lyberg, Lars},
doi = {10.1093/poq/nfq065},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/nfq065.pdf:pdf},
issn = {0033362X},
journal = {Public Opinion Quarterly},
number = {5},
pages = {849--879},
title = {{Total survey error: Past, present, and future}},
volume = {74},
year = {2010}
}
@article{Shirani-Mehr2018,
abstract = {It is well known among researchers and practitioners that election polls suffer from a variety of sampling and nonsampling errors, often collectively referred to as total survey error. Reported margins of error typically only capture sampling variability, and in particular, generally ignore nonsampling errors in defining the target population (e.g., errors due to uncertainty in who will vote). Here, we empirically analyze 4221 polls for 608 state-level presidential, senatorial, and gubernatorial elections between 1998 and 2014, all of which were conducted during the final three weeks of the campaigns. Comparing to the actual election outcomes, we find that average survey error as measured by root mean square error is approximately 3.5 percentage points, about twice as large as that implied by most reported margins of error. We decompose survey error into election-level bias and variance terms. We find that average absolute election-level bias is about 2 percentage points, indicating that polls for a given election often share a common component of error. This shared error may stem from the fact that polling organizations often face similar difficulties in reaching various subgroups of the population, and that they rely on similar screening rules when estimating who will vote. We also find that average election-level variance is higher than implied by simple random sampling, in part because polling organizations often use complex sampling designs and adjustment procedures. We conclude by discussing how these results help explain polling failures in the 2016 U.S. presidential election, and offer recommendations to improve polling practice.},
author = {Shirani-Mehr, Houshmand and Rothschild, David and Goel, Sharad and Gelman, Andrew},
doi = {10.1080/01621459.2018.1448823},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/Disentangling Bias and Variance in Election Polls.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Margin of error,Nonsampling error,Polling bias,Total survey error},
number = {522},
pages = {607--614},
title = {{Disentangling Bias and Variance in Election Polls}},
url = {https://doi.org/10.1080/01621459.2018.1448823},
volume = {113},
year = {2018}
}
@article{Park2004,
abstract = {We fit a multilevel logistic regression model for the mean of a binary response variable conditional on poststratification cells. This approach combines the modeling approach often used in small-area estimation with the population information used in poststratification (see Gelman and Little 1997, Survey Methodology 23:127-135). To validate the method, we apply it to U.S. preelection polls for 1988 and 1992, poststratified by state, region, and the usual demographic variables. We evaluate the model by comparing it to state-level election outcomes. The multilevel model outperforms more commonly used models in political science. We envision the most important usage of this method to be not forecasting elections but estimating public opinion on a variety of issues at the state level. {\textcopyright} Society for Political Methodology 2004; all rights reserved.},
author = {Park, David K. and Gelman, Andrew and Bafumi, Joseph},
doi = {10.1093/pan/mph024},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Park, Gelman, Bafumi - 2004 - Bayesian multilevel estimation with poststratification State-level estimates from national polls.pdf:pdf},
issn = {10471987},
journal = {Political Analysis},
number = {4},
pages = {375--385},
title = {{Bayesian multilevel estimation with poststratification: State-level estimates from national polls}},
volume = {12},
year = {2004}
}
@article{Biemer2010,
abstract = {The total survey error (TSE) paradigm provides a theoretical framework for optimizing surveys by maximizing data quality within budgetary constraints. In this article, the TSE paradigm is viewed as part of a much larger design strategy that seeks to optimize surveys by maximizing total survey quality; i.e., quality more broadly defined to include user-specified dimensions of quality. Survey methodology, viewed within this larger framework, alters our perspectives on the survey design, implementation, and evaluation. As an example, although a major objective of survey design is to maximize accuracy subject to costs and timeliness constraints, the survey budget must also accommodate additional objectives related to relevance, accessibility, interpretability, comparability, coherence, and completeness that are critical to a survey's "fitness for use." The article considers how the total survey quality approach can be extended beyond survey design to include survey implementation and evaluation. In doing so, the "fitness for use" perspective is shown to influence decisions regarding how to reduce survey error during design implementation and what sources of error should be evaluated in order to assess the survey quality today and to prepare for the surveys of the future. {\textcopyright} The Author 2011.},
author = {Biemer, Paul P.},
doi = {10.1093/poq/nfq058},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/nfq058.pdf:pdf},
issn = {0033362X},
journal = {Public Opinion Quarterly},
number = {5},
pages = {817--848},
title = {{Total survey error: Design, implementation, and evaluation}},
volume = {74},
year = {2010}
}
@article{Wang2015,
abstract = {Election forecasts have traditionally been based on representative polls, in which randomly sampled individuals are asked who they intend to vote for. While representative polling has historically proven to be quite effective, it comes at considerable costs of time and money. Moreover, as response rates have declined over the past several decades, the statistical benefits of representative sampling have diminished. In this paper, we show that, with proper statistical adjustment, non-representative polls can be used to generate accurate election forecasts, and that this can often be achieved faster and at a lesser expense than traditional survey methods. We demonstrate this approach by creating forecasts from a novel and highly non-representative survey dataset: a series of daily voter intention polls for the 2012 presidential election conducted on the Xbox gaming platform. After adjusting the Xbox responses via multilevel regression and poststratification, we obtain estimates which are in line with the forecasts from leading poll analysts, which were based on aggregating hundreds of traditional polls conducted during the election cycle. We conclude by arguing that non-representative polling shows promise not only for election forecasting, but also for measuring public opinion on a broad range of social, economic and cultural issues.},
author = {Wang, Wei and Rothschild, David and Goel, Sharad and Gelman, Andrew},
doi = {10.1016/j.ijforecast.2014.06.001},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2015 - Forecasting elections with non-representative polls.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Election forecasting,Multilevel regression and poststratification,Non-representative polling},
month = {jul},
number = {3},
pages = {980--991},
publisher = {Elsevier B.V.},
title = {{Forecasting elections with non-representative polls}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2014.06.001 https://linkinghub.elsevier.com/retrieve/pii/S0169207014000879},
volume = {31},
year = {2015}
}
@article{Couper2017,
abstract = {This review focuses on recent methodological and technological developments in survey data collection. Surveys are facing unprecedented challenges from both societal and technological changes. Against this backdrop, I review the survey profession's response to these challenges and developments to enhance and extend the survey tool. I discuss the decline in random digit dialing and the rise of address-based sampling, along with the corresponding shift from telephone surveys to self-administered (mail and or Web) modes. I discuss the rise in nonprobability sampling approaches, especially those associated with online data collection. I also review so-called big data alternatives to surveys. Finally, I discuss a number of recent methodological and technological trends designed to modernize the survey method. I conclude that although they face a number of major challenges, surveys remain a robust and flexible method for collecting data on, and making inference to, populations.},
author = {Couper, Mick P.},
doi = {10.1146/annurev-soc-060116-053613},
file = {:home/nandan/Downloads/_papers/annurev-soc-060116-053613.pdf:pdf},
issn = {03600572},
journal = {Annual Review of Sociology},
keywords = {Adaptive design,Address-based sampling,Mail surveys,Mixed-mode data collection,Nonprobability methods,Random digit dialing,Responsive design,Survey mode,Telephone surveys,Web surveys},
pages = {121--145},
title = {{New developments in survey data collection}},
volume = {43},
year = {2017}
}
@article{Dutwin2017,
abstract = {Nonprobability samples have gained mass popularity and use in many research circles, including market research and some political research. One justification for the use of nonprobability samples is that low response rate probability surveys have nothing significant to offer over and above a "well built" nonprobability sample. Utilizing an elemental approach, we compare a range of samples, weighting, and modeling procedures in an analysis that evaluates the estimated bias of various cross-tabulations of core demographics. Specifically, we compare a battery of bias related metrics for nonprobability panels, dualframe telephone samples, and a high-quality in-person sample. Results indicate that there is roughly a linear trend, with nonprobability samples attaining the greatest estimated bias, and the in-person sample, the lowest. Results also indicate that the bias estimates vary widely for the nonprobability samples compared to either the telephone or in-person samples, which themselves tend to have consistently smaller amounts of estimated bias. Specifically, both weighted and unweighted dual-frame telephone samples were found to have about half the estimated bias compared to analogous nonprobability samples. Advanced techniques such as propensity weighting and sample matching did not improve these measures, and in some cases made matters worse. Implications for "fit for purpose" in survey research are discussed given these findings.},
author = {Dutwin, David and Buskirk, Trent D.},
doi = {10.1093/poq/nfw061},
file = {:home/nandan/Downloads/_papers/oup-accepted-manuscript-2017.pdf:pdf},
issn = {15375331},
journal = {Public Opinion Quarterly},
number = {January},
pages = {213--249},
title = {{Apples to Oranges or Gala versus Golden Delicious?}},
volume = {81},
year = {2017}
}
@article{Schneider2019,
abstract = {In this article, we explore the use of Facebook targeted advertisements for the collection of survey data. We illustrate the potential of survey sampling and recruitment on Facebook through the example of building a large employee–employer linked data set as part of The Shift Project. We describe the workflow process of targeting, creating, and purchasing survey recruitment advertisements on Facebook. We address concerns about sample selectivity and apply poststratification weighting techniques to adjust for differences between our sample and that of “gold standard” data sources. We then compare univariate and multivariate relationships in the Shift data against the Current Population Survey and the National Longitudinal Survey of Youth 1997. Finally, we provide an example of the utility of the firm-level nature of the data by showing how firm-level gender composition is related to wages. We conclude by discussing some important remaining limitations of the Facebook approach, as well as highlighting some unique strengths of the Facebook targeted advertisement approach, including the ability for rapid data collection in response to research opportunities, rich and flexible sample targeting capabilities, and low cost, and we suggest broader applications of this technique.},
author = {Schneider, Daniel and Harknett, Kristen},
doi = {10.1177/0049124119882477},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schneider, Harknett - 2019 - What's to Like Facebook as a Tool for Survey Data Collection.pdf:pdf},
issn = {15528294},
journal = {Sociological Methods and Research},
keywords = {firm-level data,gender,nonprobability sampling,survey methods,work},
title = {{What's to Like? Facebook as a Tool for Survey Data Collection}},
year = {2019}
}
@article{Heffetz2019,
abstract = {How high is unemployment? How low is labor force participation? Is obesity more prevalent among men? How large are household expenditures? We study the sources of the relevant official statistics-The Current Population Survey, the Behavioral Risk Factor Surveillance System, and the Consumer Expenditure Survey-And find that the answers depend on whether we look at easy-or at difficult-To-reach respondents, measured by the number of call and visit attempts made by interviewers. A challenge to the (conditionally-)random-nonresponse assumption, these findings empirically substantiate the theoretical warning against making population-wide estimates from surveys with low response rates.},
author = {Heffetz, Ori and Reeves, Daniel B.},
doi = {10.1162/rest_a_00748},
file = {:home/nandan/Downloads/_papers/rest_a_00748.pdf:pdf},
issn = {15309142},
journal = {Review of Economics and Statistics},
number = {1},
pages = {176--191},
title = {{Difficulty of Reaching Respondents and Nonresponse Bias: Evidence from Large Government Surveys}},
volume = {101},
year = {2019}
}
@article{Deville1992,
abstract = {This article investigates estimation of finite population totals in the presence of univariate or multivariate auxiliary information. Estimation is equivalent to attaching weights to the survey data. We focus attention on the several weighting systems that can be associated with a given amount of auxiliary information and derive a weighting system with the aid of a distance measure and a set of calibration equations. We briefly mention an application to the case in which the information consists of known marginal counts in a two- or multi-way table, known as generalized raking. The general regression estimator (GREG) was conceived with multivariate auxiliary information in mind. Ordinarily, this estimator is justified by a regression relationship between the study variable y and the auxiliary vector x. But we note that the GREG can be derived by a different route by focusing instead on the weights. The ordinary sampling weights of the kth observation is 1/$\pi$k, where $\pi$k is the inclusion probability of k. We show that the weights implied by the GREG are as close as possible, according to a given distance measure, to the 1/$\pi$k while respecting side conditions called calibration equations. These state that the sample sum of the weighted auxiliary variable values must equal the known population total for that auxiliary variable. That is, the calibrated weights must give perfect estimates when applied to each auxiliary variables and the study variable means that the weights that perform well for the auxiliary variable also should perform well for the study variable. The GREG uses the auxiliary information efficiently, so the estimates are precise; however, the individual weights are not always without reproach. For example, negative weights can occur, and in some applications this does not make sense. It is natural to seek the root of the dissatisfaction in the underlying distance measure. Consequently, we allow alternative distance measures that satisfy only a set of minimal requirements. Each distance measure leads, via the calibration equations, to a specific weighting system and thereby to a new estimator. These estimators form a family of calibration estimators. We show that the GREG is a first approximation to all other members of the family; all are asymptotically equivalent to the GREG, and the variance estimator already known for the GREG is recommended for use in any other member of the family. Numerical features of the weights and ease of computation become more than anything else the bases for choosing between the estimators. The reasoning is applied to calibration on known marginals of a two-way frequency table. Our family of distance measures leads in this case to a family of generalized raking procedures, of which classical raking ratio is one.},
author = {Deville, Jean-Claude and S{\"{a}}rndal, Carl-Erik},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/deville1992.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {418},
pages = {376--382},
title = {{in Survey Sampling Calibration Estimators}},
url = {http://www.jstor.org/stable/2290268},
volume = {87},
year = {1992}
}
@article{Cooper1964,
author = {Cooper, Sanford L},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/cooper1964.pdf:pdf},
journal = {Journal of Marketing Research},
number = {4},
pages = {45--48},
title = {{Random Sampling by Telephone - An Improved Method}},
volume = {1},
year = {1964}
}
@article{Sudman1973,
author = {Sudman, Seymour},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/sudman1973.pdf:pdf},
journal = {Journal of Marketing Research},
number = {2},
pages = {204--207},
title = {{Uses of Telephone Directories for Survey Sampling}},
volume = {10},
year = {1973}
}
@article{Glasser1972,
author = {Glasser, Gerald J and Metzger, Gale D},
file = {:home/nandan/Dropbox/Research/Random Papers to Read/glasser1972.pdf:pdf},
journal = {Journal of Marketing Research},
number = {1},
pages = {59--64},
title = {{Random-Digit Dialing as a Method of Telephone Sampling}},
volume = {9},
year = {1972}
}
@article{Mercer2018,
author = {Mercer, Andrew and Lau, Arnold and Kennedy, Courtney},
file = {:home/nandan/Downloads/_papers/Weighting-Online-Opt-In-Samples.pdf:pdf},
journal = {Pew Research Center},
title = {{For Weighting Online Opt-In Samples, What Matters Most?}},
year = {2018}
}
@book{Groves2010,
author = {Groves, Robert M. and Singer, Eleanor and Lepkowski, James M. and Heeringa, Steven G. and Alwin, Duane F.},
booktitle = {A Telescope on Society: Survey Research and Social Science at the University of Michigan and Beyond},
doi = {10.4324/9780429314254-2},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Groves et al. - 2010 - Survey methodology.pdf:pdf},
isbn = {0472098489},
issn = {0162-1459},
pages = {21--64},
title = {{Survey methodology}},
year = {2010}
}
@article{Kolenikov2016,
author = {Kolenikov, Stas J.},
doi = {10.29115/SP-2016-0014},
file = {:home/nandan/Downloads/_papers/2809-post-stratification-or-non-response-adjustment.pdf:pdf},
issn = {2168-0094},
journal = {Survey Practice},
keywords = {10,29115,calibration,doi,https,non-response adjustment,org,raking,sp-2016-0014,weighting},
month = {aug},
number = {3},
pages = {1--12},
title = {{Post-stratification or a non-response adjustment?}},
url = {https://surveypractice.scholasticahq.com/article/2809-post-stratification-or-non-response-adjustment},
volume = {9},
year = {2016}
}
@book{Fowler2014,
abstract = {The Fourth Edition of the bestselling Survey Research Methods presents the very latest methodological knowledge on surveys. Author Floyd J. Fowler Jr. provides students and researchers who want to collect, analyze, or read about survey data with a sound basis for evaluating how each aspect of a survey can affect its precision, accuracy, and credibility. The Fourth Edition has been updated in four primary ways: it much more prominently addresses the growth of the Internet for data collection and the subsequent rapid expansion of online survey usage; it addresses the precipitous drop in response rates for telephone surveys, particularly those based on random-digit dialing; it offers new and expanded coverage monitoring the continued improvement in techniques for presurvey evaluation of questions; and it addresses the growing role of individual cell phone in addition - and often instead of - household landlines. Two new chapters, “The Nature of Error in Surveys” and “Issues in Analyzing Survey Data,” further emphasize the importance of minimizing nonsampling errors through superior question design, quality interviewing, and high response rates. Key FeaturesCovers the expansion of cell phone use and legislation regarding them; this offers survey researchers guidance as to policy implications and practical application Expands the coverage of web-based and online surveys as well as the latest resources available to the beginning and expert researcher Offers in-depth discussion of non-response and sample size issues, especially as they relate to powerFocuses on data analysis, especially with regard to bivariate and multivariate approaches. Fowler walks students and researchers through the various types of analyses one would do once the data are ready to analyze. Provides a list of strengths and weaknesses for each of the different types of survey data collection, including the more recent web-based approachesIncludes updated references and survey examples that offer various levels of students and researchers other exemplary literature and modelsSurvey Research Methods, Fourth Edition gives compact, yet comprehensive coverage, making it an ideal companion or beginning text. Praise for Floyd J. Fowler, Jr. and the previous edition:“Fowler is smart, straightforward, and sensible in writing about research methodology. Students have a lot to gain from his wisdom and experience.” —Mark Berends, Vanderbilt University},
author = {Fowler, Floyd J.},
booktitle = {Sage Publications, Inc},
file = {:home/nandan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fowler - 2014 - Survey Research Methods (5th edition).pdf:pdf},
isbn = {9781452259000},
pages = {185},
title = {{Survey Research Methods (5th edition)}},
year = {2014}
}
@article{Ambel2021,
author = {Ambel, Alemayehu and McGee, Kevin and Tsegay, Asmelash},
file = {:home/nandan/Downloads/_papers/Reducing-Bias-in-Phone-Survey-Samples-Effectiveness-of-Reweighting-Techniques-Using-Face-to-Face-Surveys-as-Frames-in-Four-African-Countries.pdf:pdf},
number = {May},
title = {{Reducing Bias in Phone Survey Samples Effectiveness of Reweighting Techniques Using Face-to-Face Surveys as Frames in Four African Countries}},
year = {2021}
}
